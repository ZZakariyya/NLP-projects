{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.']*2 + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(2, keepdim=True) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daill.\n",
      "jalantestian.\n",
      "na.\n",
      "sudaeveigh.\n",
      "diren.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0  \n",
    "    while True:\n",
    "        p = P[ix1, ix2] \n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:  \n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3  \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.']*2 + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs, dtype=torch.long)\n",
    "ys = torch.tensor(ys, dtype=torch.long)\n",
    "num = xs.shape[0]\n",
    "\n",
    "xenc = torch.nn.functional.one_hot(xs, num_classes=27).float().view(num, -1)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3861, grad_fn=<NegBackward0>)\n",
      "tensor(2.3851, grad_fn=<NegBackward0>)\n",
      "tensor(2.3842, grad_fn=<NegBackward0>)\n",
      "tensor(2.3834, grad_fn=<NegBackward0>)\n",
      "tensor(2.3825, grad_fn=<NegBackward0>)\n",
      "tensor(2.3817, grad_fn=<NegBackward0>)\n",
      "tensor(2.3809, grad_fn=<NegBackward0>)\n",
      "tensor(2.3801, grad_fn=<NegBackward0>)\n",
      "tensor(2.3794, grad_fn=<NegBackward0>)\n",
      "tensor(2.3787, grad_fn=<NegBackward0>)\n",
      "tensor(2.3780, grad_fn=<NegBackward0>)\n",
      "tensor(2.3773, grad_fn=<NegBackward0>)\n",
      "tensor(2.3766, grad_fn=<NegBackward0>)\n",
      "tensor(2.3760, grad_fn=<NegBackward0>)\n",
      "tensor(2.3754, grad_fn=<NegBackward0>)\n",
      "tensor(2.3748, grad_fn=<NegBackward0>)\n",
      "tensor(2.3742, grad_fn=<NegBackward0>)\n",
      "tensor(2.3736, grad_fn=<NegBackward0>)\n",
      "tensor(2.3731, grad_fn=<NegBackward0>)\n",
      "tensor(2.3727, grad_fn=<NegBackward0>)\n",
      "tensor(2.3725, grad_fn=<NegBackward0>)\n",
      "tensor(2.3733, grad_fn=<NegBackward0>)\n",
      "tensor(2.3762, grad_fn=<NegBackward0>)\n",
      "tensor(2.3885, grad_fn=<NegBackward0>)\n",
      "tensor(2.4001, grad_fn=<NegBackward0>)\n",
      "tensor(2.4626, grad_fn=<NegBackward0>)\n",
      "tensor(2.3887, grad_fn=<NegBackward0>)\n",
      "tensor(2.4092, grad_fn=<NegBackward0>)\n",
      "tensor(2.4019, grad_fn=<NegBackward0>)\n",
      "tensor(2.4587, grad_fn=<NegBackward0>)\n",
      "tensor(2.3873, grad_fn=<NegBackward0>)\n",
      "tensor(2.4059, grad_fn=<NegBackward0>)\n",
      "tensor(2.3997, grad_fn=<NegBackward0>)\n",
      "tensor(2.4547, grad_fn=<NegBackward0>)\n",
      "tensor(2.3865, grad_fn=<NegBackward0>)\n",
      "tensor(2.4065, grad_fn=<NegBackward0>)\n",
      "tensor(2.3980, grad_fn=<NegBackward0>)\n",
      "tensor(2.4516, grad_fn=<NegBackward0>)\n",
      "tensor(2.3856, grad_fn=<NegBackward0>)\n",
      "tensor(2.4064, grad_fn=<NegBackward0>)\n",
      "tensor(2.3965, grad_fn=<NegBackward0>)\n",
      "tensor(2.4491, grad_fn=<NegBackward0>)\n",
      "tensor(2.3846, grad_fn=<NegBackward0>)\n",
      "tensor(2.4061, grad_fn=<NegBackward0>)\n",
      "tensor(2.3951, grad_fn=<NegBackward0>)\n",
      "tensor(2.4471, grad_fn=<NegBackward0>)\n",
      "tensor(2.3837, grad_fn=<NegBackward0>)\n",
      "tensor(2.4057, grad_fn=<NegBackward0>)\n",
      "tensor(2.3939, grad_fn=<NegBackward0>)\n",
      "tensor(2.4454, grad_fn=<NegBackward0>)\n",
      "tensor(2.3828, grad_fn=<NegBackward0>)\n",
      "tensor(2.4051, grad_fn=<NegBackward0>)\n",
      "tensor(2.3928, grad_fn=<NegBackward0>)\n",
      "tensor(2.4440, grad_fn=<NegBackward0>)\n",
      "tensor(2.3819, grad_fn=<NegBackward0>)\n",
      "tensor(2.4045, grad_fn=<NegBackward0>)\n",
      "tensor(2.3918, grad_fn=<NegBackward0>)\n",
      "tensor(2.4427, grad_fn=<NegBackward0>)\n",
      "tensor(2.3811, grad_fn=<NegBackward0>)\n",
      "tensor(2.4039, grad_fn=<NegBackward0>)\n",
      "tensor(2.3908, grad_fn=<NegBackward0>)\n",
      "tensor(2.4416, grad_fn=<NegBackward0>)\n",
      "tensor(2.3803, grad_fn=<NegBackward0>)\n",
      "tensor(2.4033, grad_fn=<NegBackward0>)\n",
      "tensor(2.3900, grad_fn=<NegBackward0>)\n",
      "tensor(2.4406, grad_fn=<NegBackward0>)\n",
      "tensor(2.3795, grad_fn=<NegBackward0>)\n",
      "tensor(2.4026, grad_fn=<NegBackward0>)\n",
      "tensor(2.3891, grad_fn=<NegBackward0>)\n",
      "tensor(2.4397, grad_fn=<NegBackward0>)\n",
      "tensor(2.3788, grad_fn=<NegBackward0>)\n",
      "tensor(2.4020, grad_fn=<NegBackward0>)\n",
      "tensor(2.3884, grad_fn=<NegBackward0>)\n",
      "tensor(2.4388, grad_fn=<NegBackward0>)\n",
      "tensor(2.3781, grad_fn=<NegBackward0>)\n",
      "tensor(2.4014, grad_fn=<NegBackward0>)\n",
      "tensor(2.3877, grad_fn=<NegBackward0>)\n",
      "tensor(2.4381, grad_fn=<NegBackward0>)\n",
      "tensor(2.3775, grad_fn=<NegBackward0>)\n",
      "tensor(2.4008, grad_fn=<NegBackward0>)\n",
      "tensor(2.3870, grad_fn=<NegBackward0>)\n",
      "tensor(2.4374, grad_fn=<NegBackward0>)\n",
      "tensor(2.3769, grad_fn=<NegBackward0>)\n",
      "tensor(2.4003, grad_fn=<NegBackward0>)\n",
      "tensor(2.3864, grad_fn=<NegBackward0>)\n",
      "tensor(2.4367, grad_fn=<NegBackward0>)\n",
      "tensor(2.3763, grad_fn=<NegBackward0>)\n",
      "tensor(2.3997, grad_fn=<NegBackward0>)\n",
      "tensor(2.3858, grad_fn=<NegBackward0>)\n",
      "tensor(2.4361, grad_fn=<NegBackward0>)\n",
      "tensor(2.3757, grad_fn=<NegBackward0>)\n",
      "tensor(2.3992, grad_fn=<NegBackward0>)\n",
      "tensor(2.3853, grad_fn=<NegBackward0>)\n",
      "tensor(2.4356, grad_fn=<NegBackward0>)\n",
      "tensor(2.3752, grad_fn=<NegBackward0>)\n",
      "tensor(2.3987, grad_fn=<NegBackward0>)\n",
      "tensor(2.3848, grad_fn=<NegBackward0>)\n",
      "tensor(2.4350, grad_fn=<NegBackward0>)\n",
      "tensor(2.3747, grad_fn=<NegBackward0>)\n",
      "tensor(2.3982, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -torch.log(probs[torch.arange(num), ys]).mean()\n",
    "    print(loss)\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -50 * W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camyn.\n",
      "cor.\n",
      "aryeshaumiylielyna.\n",
      "aat.\n",
      "raylaheree.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    out = []\n",
    "  \n",
    "    context = [0, 0]\n",
    "    while True:\n",
    "        xenc = torch.cat(\n",
    "            [torch.nn.functional.one_hot(torch.tensor([ix]), num_classes=27).float() \n",
    "             for ix in context],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs.squeeze(0), num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know if it improved much. I'd say it is close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
