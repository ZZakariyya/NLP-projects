{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.']*2 + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N + 1).float()\n",
    "P /= P.sum(2, keepdim=True) + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daill.\n",
      "jalantestian.\n",
      "na.\n",
      "sudaeveigh.\n",
      "diren.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0  \n",
    "    while True:\n",
    "        p = P[ix1, ix2] \n",
    "        ix3 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix3])\n",
    "        if ix3 == 0:  \n",
    "            break\n",
    "        ix1, ix2 = ix2, ix3  \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.']*2 + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs, dtype=torch.long)\n",
    "ys = torch.tensor(ys, dtype=torch.long)\n",
    "num = xs.shape[0]\n",
    "\n",
    "xenc = torch.nn.functional.one_hot(xs, num_classes=27).float().view(num, -1)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3861, grad_fn=<NegBackward0>)\n",
      "tensor(2.3851, grad_fn=<NegBackward0>)\n",
      "tensor(2.3842, grad_fn=<NegBackward0>)\n",
      "tensor(2.3834, grad_fn=<NegBackward0>)\n",
      "tensor(2.3825, grad_fn=<NegBackward0>)\n",
      "tensor(2.3817, grad_fn=<NegBackward0>)\n",
      "tensor(2.3809, grad_fn=<NegBackward0>)\n",
      "tensor(2.3801, grad_fn=<NegBackward0>)\n",
      "tensor(2.3794, grad_fn=<NegBackward0>)\n",
      "tensor(2.3787, grad_fn=<NegBackward0>)\n",
      "tensor(2.3780, grad_fn=<NegBackward0>)\n",
      "tensor(2.3773, grad_fn=<NegBackward0>)\n",
      "tensor(2.3766, grad_fn=<NegBackward0>)\n",
      "tensor(2.3760, grad_fn=<NegBackward0>)\n",
      "tensor(2.3754, grad_fn=<NegBackward0>)\n",
      "tensor(2.3748, grad_fn=<NegBackward0>)\n",
      "tensor(2.3742, grad_fn=<NegBackward0>)\n",
      "tensor(2.3736, grad_fn=<NegBackward0>)\n",
      "tensor(2.3731, grad_fn=<NegBackward0>)\n",
      "tensor(2.3727, grad_fn=<NegBackward0>)\n",
      "tensor(2.3725, grad_fn=<NegBackward0>)\n",
      "tensor(2.3733, grad_fn=<NegBackward0>)\n",
      "tensor(2.3762, grad_fn=<NegBackward0>)\n",
      "tensor(2.3885, grad_fn=<NegBackward0>)\n",
      "tensor(2.4001, grad_fn=<NegBackward0>)\n",
      "tensor(2.4626, grad_fn=<NegBackward0>)\n",
      "tensor(2.3887, grad_fn=<NegBackward0>)\n",
      "tensor(2.4092, grad_fn=<NegBackward0>)\n",
      "tensor(2.4019, grad_fn=<NegBackward0>)\n",
      "tensor(2.4587, grad_fn=<NegBackward0>)\n",
      "tensor(2.3873, grad_fn=<NegBackward0>)\n",
      "tensor(2.4059, grad_fn=<NegBackward0>)\n",
      "tensor(2.3997, grad_fn=<NegBackward0>)\n",
      "tensor(2.4547, grad_fn=<NegBackward0>)\n",
      "tensor(2.3865, grad_fn=<NegBackward0>)\n",
      "tensor(2.4065, grad_fn=<NegBackward0>)\n",
      "tensor(2.3980, grad_fn=<NegBackward0>)\n",
      "tensor(2.4516, grad_fn=<NegBackward0>)\n",
      "tensor(2.3856, grad_fn=<NegBackward0>)\n",
      "tensor(2.4064, grad_fn=<NegBackward0>)\n",
      "tensor(2.3965, grad_fn=<NegBackward0>)\n",
      "tensor(2.4491, grad_fn=<NegBackward0>)\n",
      "tensor(2.3846, grad_fn=<NegBackward0>)\n",
      "tensor(2.4061, grad_fn=<NegBackward0>)\n",
      "tensor(2.3951, grad_fn=<NegBackward0>)\n",
      "tensor(2.4471, grad_fn=<NegBackward0>)\n",
      "tensor(2.3837, grad_fn=<NegBackward0>)\n",
      "tensor(2.4057, grad_fn=<NegBackward0>)\n",
      "tensor(2.3939, grad_fn=<NegBackward0>)\n",
      "tensor(2.4454, grad_fn=<NegBackward0>)\n",
      "tensor(2.3828, grad_fn=<NegBackward0>)\n",
      "tensor(2.4051, grad_fn=<NegBackward0>)\n",
      "tensor(2.3928, grad_fn=<NegBackward0>)\n",
      "tensor(2.4440, grad_fn=<NegBackward0>)\n",
      "tensor(2.3819, grad_fn=<NegBackward0>)\n",
      "tensor(2.4045, grad_fn=<NegBackward0>)\n",
      "tensor(2.3918, grad_fn=<NegBackward0>)\n",
      "tensor(2.4427, grad_fn=<NegBackward0>)\n",
      "tensor(2.3811, grad_fn=<NegBackward0>)\n",
      "tensor(2.4039, grad_fn=<NegBackward0>)\n",
      "tensor(2.3908, grad_fn=<NegBackward0>)\n",
      "tensor(2.4416, grad_fn=<NegBackward0>)\n",
      "tensor(2.3803, grad_fn=<NegBackward0>)\n",
      "tensor(2.4033, grad_fn=<NegBackward0>)\n",
      "tensor(2.3900, grad_fn=<NegBackward0>)\n",
      "tensor(2.4406, grad_fn=<NegBackward0>)\n",
      "tensor(2.3795, grad_fn=<NegBackward0>)\n",
      "tensor(2.4026, grad_fn=<NegBackward0>)\n",
      "tensor(2.3891, grad_fn=<NegBackward0>)\n",
      "tensor(2.4397, grad_fn=<NegBackward0>)\n",
      "tensor(2.3788, grad_fn=<NegBackward0>)\n",
      "tensor(2.4020, grad_fn=<NegBackward0>)\n",
      "tensor(2.3884, grad_fn=<NegBackward0>)\n",
      "tensor(2.4388, grad_fn=<NegBackward0>)\n",
      "tensor(2.3781, grad_fn=<NegBackward0>)\n",
      "tensor(2.4014, grad_fn=<NegBackward0>)\n",
      "tensor(2.3877, grad_fn=<NegBackward0>)\n",
      "tensor(2.4381, grad_fn=<NegBackward0>)\n",
      "tensor(2.3775, grad_fn=<NegBackward0>)\n",
      "tensor(2.4008, grad_fn=<NegBackward0>)\n",
      "tensor(2.3870, grad_fn=<NegBackward0>)\n",
      "tensor(2.4374, grad_fn=<NegBackward0>)\n",
      "tensor(2.3769, grad_fn=<NegBackward0>)\n",
      "tensor(2.4003, grad_fn=<NegBackward0>)\n",
      "tensor(2.3864, grad_fn=<NegBackward0>)\n",
      "tensor(2.4367, grad_fn=<NegBackward0>)\n",
      "tensor(2.3763, grad_fn=<NegBackward0>)\n",
      "tensor(2.3997, grad_fn=<NegBackward0>)\n",
      "tensor(2.3858, grad_fn=<NegBackward0>)\n",
      "tensor(2.4361, grad_fn=<NegBackward0>)\n",
      "tensor(2.3757, grad_fn=<NegBackward0>)\n",
      "tensor(2.3992, grad_fn=<NegBackward0>)\n",
      "tensor(2.3853, grad_fn=<NegBackward0>)\n",
      "tensor(2.4356, grad_fn=<NegBackward0>)\n",
      "tensor(2.3752, grad_fn=<NegBackward0>)\n",
      "tensor(2.3987, grad_fn=<NegBackward0>)\n",
      "tensor(2.3848, grad_fn=<NegBackward0>)\n",
      "tensor(2.4350, grad_fn=<NegBackward0>)\n",
      "tensor(2.3747, grad_fn=<NegBackward0>)\n",
      "tensor(2.3982, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True)\n",
    "    loss = -torch.log(probs[torch.arange(num), ys]).mean()\n",
    "    print(loss)\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -50 * W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camyn.\n",
      "cor.\n",
      "aryeshaumiylielyna.\n",
      "aat.\n",
      "raylaheree.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): \n",
    "    out = []\n",
    "  \n",
    "    context = [0, 0]\n",
    "    while True:\n",
    "        xenc = torch.cat(\n",
    "            [torch.nn.functional.one_hot(torch.tensor([ix]), num_classes=27).float() \n",
    "             for ix in context],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(probs.squeeze(0), num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know if it improved much. In theory it should, but our both models are so simple. Thus both kinda returns somewhat useless outputs. Currently I don't know any metric I can compare them with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)\n",
    "n = len(words)\n",
    "train_words = words[:int(0.8*n)]\n",
    "dev_words = words[int(0.8*n):int(0.9*n)]\n",
    "test_words = words[int(0.9*n):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(word_list):\n",
    "    xs, ys = [], []\n",
    "    for w in word_list:\n",
    "        chs = ['.']*2 + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            xs.append((ix1, ix2))\n",
    "            ys.append(ix3)\n",
    "    return torch.tensor(xs), torch.tensor(ys)\n",
    "\n",
    "X_train, y_train = create_dataset(train_words)\n",
    "X_dev, y_dev = create_dataset(dev_words)\n",
    "X_test, y_test = create_dataset(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        xenc = torch.nn.functional.one_hot(X, num_classes=27).float().view(-1, 54)\n",
    "        logits = xenc @ model\n",
    "        loss = -torch.log(logits.softmax(1)[torch.arange(len(y)), y]).mean()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train=2.3872, dev=2.4267, test=2.4237\n",
      "Step 10: train=2.3899, dev=2.4452, test=2.4415\n",
      "Step 20: train=2.3820, dev=2.4209, test=2.4177\n",
      "Step 30: train=2.3859, dev=2.4418, test=2.4378\n",
      "Step 40: train=2.3783, dev=2.4170, test=2.4135\n",
      "Step 50: train=2.3829, dev=2.4392, test=2.4349\n",
      "Step 60: train=2.3755, dev=2.4142, test=2.4104\n",
      "Step 70: train=2.3806, dev=2.4371, test=2.4325\n",
      "Step 80: train=2.3734, dev=2.4123, test=2.4082\n",
      "Step 90: train=2.3787, dev=2.4354, test=2.4306\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    xenc = torch.nn.functional.one_hot(X_train, num_classes=27).float().view(-1, 54)\n",
    "    logits = xenc @ W\n",
    "    probs = logits.softmax(1)\n",
    "    loss = -torch.log(probs[torch.arange(len(y_train)), y_train]).mean()\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -50 * W.grad\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        train_loss = loss.item()\n",
    "        dev_loss = evaluate(W, X_dev, y_dev)\n",
    "        test_loss = evaluate(W, X_test, y_test)\n",
    "        print(f'Step {k}: train={train_loss:.4f}, dev={dev_loss:.4f}, test={test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated names:\n",
      "trasn\n",
      "ha\n",
      "ri\n",
      "halayly\n",
      "gvavekhtaere\n"
     ]
    }
   ],
   "source": [
    "def generate_name(model, num_samples=5):\n",
    "    for _ in range(num_samples):\n",
    "        out = []\n",
    "        context = [0, 0] \n",
    "        \n",
    "        while True:\n",
    "            xenc = torch.nn.functional.one_hot(torch.tensor(context), num_classes=27).float().view(1, -1)\n",
    "            logits = xenc @ model\n",
    "            probs = logits.softmax(1)\n",
    "            \n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "            out.append(itos[ix])\n",
    "            \n",
    "            context = context[1:] + [ix]\n",
    "            if ix == 0:  \n",
    "                break\n",
    "                \n",
    "        print(''.join(out[:-1]))  \n",
    "\n",
    "\n",
    "print(\"Generated names:\")\n",
    "generate_name(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.1: train=2.3842, dev=2.3907\n",
      "Alpha=0.5: train=2.3855, dev=2.3924\n",
      "Alpha=1.0: train=2.3914, dev=2.3995\n",
      "Alpha=2.0: train=2.3870, dev=2.3953\n",
      "Alpha=5.0: train=2.4357, dev=2.4496\n",
      "Alpha=10.0: train=2.3864, dev=2.3924\n",
      "Alpha=100.0: train=2.3841, dev=2.3924\n",
      "Alpha=1000.0: train=2.3853, dev=2.3929\n",
      "Alpha=10000.0: train=2.3862, dev=2.3946\n"
     ]
    }
   ],
   "source": [
    "smoothing_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "results = []\n",
    "\n",
    "for alpha in smoothing_values:\n",
    "    N = torch.zeros((27, 27, 27), dtype=torch.int32)\n",
    "    P = (N + alpha).float()\n",
    "    P /= P.sum(2, keepdim=True)\n",
    "    \n",
    "    W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    for k in range(100):\n",
    "        xenc = torch.nn.functional.one_hot(X_train, num_classes=27).float().view(-1, 54)\n",
    "        logits = xenc @ W\n",
    "        probs = logits.softmax(1)\n",
    "        loss = -torch.log(probs[torch.arange(len(y_train)), y_train]).mean()\n",
    "        \n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "    \n",
    "    train_loss = evaluate(W, X_train, y_train)\n",
    "    dev_loss = evaluate(W, X_dev, y_dev)\n",
    "    results.append((alpha, train_loss, dev_loss))\n",
    "    print(f'Alpha={alpha}: train={train_loss:.4f}, dev={dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.001: train=2.3884, dev=2.3957\n",
      "Alpha=0.01: train=2.3875, dev=2.3932\n",
      "Alpha=0.1: train=2.3925, dev=2.3984\n",
      "Alpha=0.5: train=2.5318, dev=2.5431\n",
      "Alpha=1.0: train=2.6020, dev=2.6127\n"
     ]
    }
   ],
   "source": [
    "smoothing_values = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in smoothing_values:\n",
    "    W = torch.randn((54, 27), generator=g, requires_grad=True)\n",
    "    \n",
    "    for k in range(100):\n",
    "        xenc = torch.nn.functional.one_hot(X_train, num_classes=27).float().view(-1, 54)\n",
    "        logits = xenc @ W\n",
    "        probs = logits.softmax(1)\n",
    "        loss = -torch.log(probs[torch.arange(len(y_train)), y_train]).mean() + alpha*(W**2).mean()\n",
    "        \n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        W.data += -50 * W.grad\n",
    "\n",
    "    train_loss = evaluate(W, X_train, y_train)\n",
    "    dev_loss = evaluate(W, X_dev, y_dev)\n",
    "    results.append((alpha, train_loss, dev_loss))\n",
    "    print(f'Alpha={alpha}: train={train_loss:.4f}, dev={dev_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried 2 different smoothing techniques, but it didnt create better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.']*2 + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        xs.append((ix1, ix2))\n",
    "        ys.append(ix3)\n",
    "xs = torch.tensor(xs, dtype=torch.long)\n",
    "ys = torch.tensor(ys, dtype=torch.long)\n",
    "num = xs.shape[0]\n",
    "\n",
    "xenc = torch.nn.functional.one_hot(xs, num_classes=27).float().view(num, -1)\n",
    "W = torch.randn((54, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = torch.nn.functional.one_hot(xs, num_classes=27).float().view(-1, 54) \n",
    "logits1 = xenc @ W  \n",
    "\n",
    "W_reshaped = W.view(2, 27, 27) \n",
    "logits3 = W_reshaped[0][xs[:,0]] + W_reshaped[1][xs[:,1]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4559,  0.3997, -0.3886,  ...,  0.7825,  0.6419, -0.1456],\n",
      "        [-0.7279, -2.6708, -0.8460,  ...,  0.6269,  2.5628,  0.6440],\n",
      "        [ 0.2446, -1.6428,  1.4619,  ..., -0.2249,  0.1742,  0.0665],\n",
      "        ...,\n",
      "        [ 1.6848, -2.9752,  0.1465,  ...,  0.0926, -0.4444, -1.6957],\n",
      "        [-0.3198,  1.0638,  1.2827,  ...,  1.4146, -0.0802,  1.5365],\n",
      "        [ 0.8131, -2.1948,  2.7385,  ...,  0.6649, -1.9760, -1.0428]],\n",
      "       grad_fn=<MmBackward0>) tensor([[ 0.4559,  0.3997, -0.3886,  ...,  0.7825,  0.6419, -0.1456],\n",
      "        [-0.7279, -2.6708, -0.8460,  ...,  0.6269,  2.5628,  0.6440],\n",
      "        [ 0.2446, -1.6428,  1.4619,  ..., -0.2249,  0.1742,  0.0665],\n",
      "        ...,\n",
      "        [ 1.6848, -2.9752,  0.1465,  ...,  0.0926, -0.4444, -1.6957],\n",
      "        [-0.3198,  1.0638,  1.2827,  ...,  1.4146, -0.0802,  1.5365],\n",
      "        [ 0.8131, -2.1948,  2.7385,  ...,  0.6649, -1.9760, -1.0428]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print (logits1, logits3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
